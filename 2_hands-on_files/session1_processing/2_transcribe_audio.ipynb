{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51caaaa7-ac22-423d-a89f-efe13e7f79ab",
   "metadata": {},
   "source": [
    "# Transcribe Audio\n",
    "\n",
    "In this script, we load the audio file and transcribe it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999c5cc1-6148-4de9-a198-c222ce682dce",
   "metadata": {},
   "source": [
    "### Option 1: Process a single file\n",
    "\n",
    "Select a single audio file as input and specify the output (JSON and text file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f931b2-7dea-4752-906c-cee5b39afa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 22:46:08.679636: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-08 22:46:08.748737: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-08 22:46:08.748791: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-08 22:46:08.751004: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-08 22:46:08.762846: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-08 22:46:09.772222: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../../../../../opt/anaconda3/envs/openwillis_3.1/lib/python3.10/site-packages/whisperx/assets/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.5.1+cu124. Bad things might happen unless you revert torch to 1.x.\n",
      "Detected language: it (0.96) in first 30s of audio...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Transcribe audio.wav\n",
    "\"\"\"\n",
    "\n",
    "import openwillis.transcribe as owt\n",
    "import json\n",
    "\n",
    "def transcribe_audio(audio_file, output_json, output_text):\n",
    "    \"\"\"Transcribe audio file using OpenWillis\"\"\"\n",
    "    try:\n",
    "        # Use OpenWillis to transcribe the audio\n",
    "        # Returns: (transcript_json, transcript_text)\n",
    "        result = owt.speech_transcription_whisper(\n",
    "            filepath=audio_file,\n",
    "            model=\"large-v2\",      # Best model, takes longer\n",
    "            compute_type=\"int16\",  # Default for CPU\n",
    "            device_type=\"cpu\",     # Use CPU \n",
    "            batch_size=16,         # Default batch size\n",
    "            hf_token=\"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\",  # Replace with your token\n",
    "            language=\"\",           # Auto-detection if not specified\n",
    "            min_speakers = 1,      # Speaker diarization possible\n",
    "            max_speakers = 1\n",
    "        )\n",
    "        \n",
    "        # Unpack the result\n",
    "        transcript_json, transcript_text = result\n",
    "        \n",
    "        # Save the JSON transcript (detailed word-by-word)\n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(transcript_json, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Save the text transcript (simple string)\n",
    "        with open(output_text, 'w', encoding='utf-8') as f:\n",
    "            f.write(transcript_text)\n",
    "        \n",
    "        print(f\" Transcription saved:\")\n",
    "        print(f\"  JSON: {output_json}\")\n",
    "        print(f\"  Text: {output_text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing {audio_file}: {e}\")\n",
    "\n",
    "# Define input and output files\n",
    "audio_file = \"output/RAVDESS/audio_only/01-01-01-01-01-01-01.wav\"     # Input audio file from previous step\n",
    "json_output = \"output/RAVDESS/transcripts/01-01-01-01-01-01-01.json\"  # Detailed transcript\n",
    "text_output = \"output/RAVDESS/transcripts/01-01-01-01-01-01-01.txt\"   # Simple text transcript\n",
    "\n",
    "# Transcribe the audio\n",
    "transcribe_audio(audio_file, json_output, text_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba3ca99-3b2f-4553-b00d-ab76439d3da3",
   "metadata": {},
   "source": [
    "### Option 2: Process all files in a folder\n",
    "\n",
    "Select a folder with audio files as input and specify your output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "043888e4-cbe2-4821-ae48-51753df5d386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Example_Video_audio.wav...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../../opt/anaconda3/envs/openwillis_3.1/lib/python3.10/site-packages/whisperx/assets/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.5.1+cu124. Bad things might happen unless you revert torch to 1.x.\n",
      " Transcription saved:\n",
      "  JSON: /mnt/nfs/data/code/openwillis/workshop_10092025/Example_JSON_Folder/Example_Video_audio_transcript.json\n",
      "  Text: /mnt/nfs/data/code/openwillis/workshop_10092025/Example_Text_Folder/Example_Video_audio_transcript.txt\n",
      "Processing Example_Audio.wav...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../../opt/anaconda3/envs/openwillis_3.1/lib/python3.10/site-packages/whisperx/assets/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.5.1+cu124. Bad things might happen unless you revert torch to 1.x.\n",
      " Transcription saved:\n",
      "  JSON: /mnt/nfs/data/code/openwillis/workshop_10092025/Example_JSON_Folder/Example_Audio_transcript.json\n",
      "  Text: /mnt/nfs/data/code/openwillis/workshop_10092025/Example_Text_Folder/Example_Audio_transcript.txt\n",
      "All files processed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Transcribe all audio files in a directory\n",
    "\"\"\"\n",
    "\n",
    "import openwillis.transcribe as owt\n",
    "import json\n",
    "import os\n",
    "\n",
    "def transcribe_audio(audio_file, output_json, output_text):\n",
    "    \"\"\"Transcribe single audio file using OpenWillis\"\"\"\n",
    "    try:\n",
    "        # Use OpenWillis to transcribe the audio\n",
    "        # Returns: (transcript_json, transcript_text)\n",
    "        result = owt.speech_transcription_whisper(\n",
    "            filepath=audio_file,\n",
    "            model=\"large-v2\",      # Best model, takes longer\n",
    "            compute_type=\"int16\",  # Default for CPU\n",
    "            device_type=\"cpu\",     # Use CPU \n",
    "            batch_size=16,         # Default batch size\n",
    "            hf_token=\"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\",  # Replace with your token\n",
    "            language=\"en\",         # Specifying recommended, else Auto-detection\n",
    "            min_speakers=1,        # Speaker diarization possible\n",
    "            max_speakers=1\n",
    "        )\n",
    "        \n",
    "        # Unpack the result\n",
    "        transcript_json, transcript_text = result\n",
    "        \n",
    "        # Save the JSON transcript \n",
    "        with open(output_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(transcript_json, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # Save the text transcript (simple string)\n",
    "        with open(output_text, 'w', encoding='utf-8') as f:\n",
    "            f.write(transcript_text)\n",
    "        \n",
    "        print(f\" Transcription saved:\")\n",
    "        print(f\"  JSON: {output_json}\")\n",
    "        print(f\"  Text: {output_text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error transcribing {audio_file}: {e}\")\n",
    "\n",
    "def process_directory(input_dir, json_output_dir, text_output_dir):\n",
    "    \"\"\"Transcribe all audio files in directory\"\"\"\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(json_output_dir, exist_ok=True)\n",
    "    os.makedirs(text_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Loop through all files in the input directory\n",
    "    for file in os.listdir(input_dir):\n",
    "        # Only process WAV files\n",
    "        if not file.endswith(\".wav\"):\n",
    "            continue\n",
    "            \n",
    "        # Create full paths for input and output files\n",
    "        audio_file = os.path.join(input_dir, file)\n",
    "        base_name = os.path.splitext(file)[0]  # Remove .wav extension\n",
    "        json_output = os.path.join(json_output_dir, f\"{base_name}_transcript.json\")\n",
    "        text_output = os.path.join(text_output_dir, f\"{base_name}_transcript.txt\")\n",
    "        \n",
    "        # Skip if transcription files already exist\n",
    "        if os.path.exists(json_output) and os.path.exists(text_output):\n",
    "            print(f\"Skipping {file}: transcription already exists\")\n",
    "            continue\n",
    "            \n",
    "        # Process the audio file\n",
    "        print(f\"Processing {file}...\")\n",
    "        transcribe_audio(audio_file, json_output, text_output)\n",
    "\n",
    "# Define input and output directories\n",
    "input_folder = \"output/RAVDESS/audio_only\" # Replace with your folder\n",
    "json_output_folder = \"output/RAVDESS/transcripts/JSON_files\"\n",
    "text_output_folder = \"output/RAVDESS/transcripts/txt_files\"\n",
    "\n",
    "# Process all audio files in the directory\n",
    "process_directory(input_folder, json_output_folder, text_output_folder)\n",
    "print(\"All files processed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
