{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e7413a2",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "In this script, we will:\n",
    "\n",
    "1. Load and preprocess the dataset.\n",
    "2. Perform UMAP (Uniform Manifold Approximation and Projection).\n",
    "3. Visualize the points in a 2-dimensional space colored by labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec01ba7",
   "metadata": {},
   "source": [
    "### Task 1: Load the CSV file, define relevant groups, and create dataframes\n",
    "\n",
    "Same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da3b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(\"../simulated_processed_data/simulated_data_combined.csv\")\n",
    "\n",
    "# Define columns\n",
    "general_info_cols = [\"id\", \"sex\", \"condition\"]\n",
    "\n",
    "audio_feature_cols = [\n",
    "    \"f0_mean\",\"f0_stddev\",\"f0_range\",\"f1_mean\",\"f1_stddev\",\"f1_range\",\n",
    "    \"f2_mean\",\"f2_stddev\",\"f2_range\",\"f3_mean\",\"f3_stddev\",\"f3_range\",\n",
    "    \"f4_mean\",\"f4_stddev\",\"f4_range\",\"loudness_mean\",\"loudness_stddev\",\"loudness_range\",\n",
    "    \"hnr_mean\",\"hnr_stddev\",\"hnr_range\",\"jitter\",\"jitter_abs\",\"jitter_rap\",\"jitter_ppq5\",\n",
    "    \"jitter_ddp\",\"shimmer\",\"shimmer_db\",\"shimmer_apq3\",\"shimmer_apq5\",\"shimmer_apq11\",\n",
    "    \"shimmer_dda\",\"gne_ratio\",\"mfcc1_mean\",\"mfcc2_mean\",\"mfcc3_mean\",\"mfcc4_mean\",\"mfcc5_mean\",\n",
    "    \"mfcc6_mean\",\"mfcc7_mean\",\"mfcc8_mean\",\"mfcc9_mean\",\"mfcc10_mean\",\"mfcc11_mean\",\"mfcc12_mean\",\n",
    "    \"mfcc13_mean\",\"mfcc14_mean\",\"mfcc1_var\",\"mfcc2_var\",\"mfcc3_var\",\"mfcc4_var\",\"mfcc5_var\",\n",
    "    \"mfcc6_var\",\"mfcc7_var\",\"mfcc8_var\",\"mfcc9_var\",\"mfcc10_var\",\"mfcc11_var\",\"mfcc12_var\",\n",
    "    \"mfcc13_var\",\"mfcc14_var\",\"cpp_mean\",\"cpp_var\",\"spir\",\"dur_med\",\"dur_mad\",\"silence_ratio\",\n",
    "    \"rel_f0_sd\",\"rel_se0_sd\"\n",
    "]\n",
    "\n",
    "video_feature_cols = [\n",
    "    \"anger_mean\",\"disgust_mean\",\"fear_mean\",\"happiness_mean\",\"sadness_mean\",\"surprise_mean\",\n",
    "    \"neutral_mean\",\"AU01_mean\",\"AU02_mean\",\"AU04_mean\",\"AU05_mean\",\"AU06_mean\",\"AU07_mean\",\n",
    "    \"AU09_mean\",\"AU10_mean\",\"AU11_mean\",\"AU12_mean\",\"AU14_mean\",\"AU15_mean\",\"AU17_mean\",\n",
    "    \"AU20_mean\",\"AU23_mean\",\"AU24_mean\",\"AU25_mean\",\"AU26_mean\",\"AU28_mean\",\"AU43_mean\",\n",
    "    \"mouth_openness_mean\",\"anger_std\",\"disgust_std\",\"fear_std\",\"happiness_std\",\"sadness_std\",\n",
    "    \"surprise_std\",\"neutral_std\",\"AU01_std\",\"AU02_std\",\"AU04_std\",\"AU05_std\",\"AU06_std\",\"AU07_std\",\n",
    "    \"AU09_std\",\"AU10_std\",\"AU11_std\",\"AU12_std\",\"AU14_std\",\"AU15_std\",\"AU17_std\",\"AU20_std\",\n",
    "    \"AU23_std\",\"AU24_std\",\"AU25_std\",\"AU26_std\",\"AU28_std\",\"AU43_std\",\"mouth_openness_std\"\n",
    "]\n",
    "\n",
    "# Create separate DataFrames\n",
    "audio_df = df[general_info_cols + audio_feature_cols]\n",
    "video_df = df[general_info_cols + video_feature_cols]\n",
    "combined_df = df[general_info_cols + audio_feature_cols + video_feature_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7823a1",
   "metadata": {},
   "source": [
    "### Task 2: Preprocess the data\n",
    "\n",
    "Follow the steps:\n",
    "\n",
    "1. Separate features and target (and drop metadata columns)\n",
    "2. Impute missing values (sklearn - SimpleImputer)\n",
    "3. Scale features (sklearn - StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e88f1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, target, metadata_cols=[\"id\", \"condition\",\"sex\"]):\n",
    "    \"\"\"\n",
    "    Preprocess dataset for modeling: drop metadata, separate target, impute missing values, and scale features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataset\n",
    "        target (str): Name of the target column\n",
    "        metadata_cols (list, optional): Columns to drop\n",
    "\n",
    "    Returns:\n",
    "        X_scaled (np.ndarray): Scaled feature matrix\n",
    "        y (pd.Series): Target labels\n",
    "    \"\"\"\n",
    "      \n",
    "    # Separate target and features\n",
    "    y = df[target]\n",
    "    X = df.drop(columns=metadata_cols)\n",
    "\n",
    "    # Impute missing values\n",
    "    imputer = SimpleImputer(strategy=\"mean\")\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "    return X_scaled, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3e59c",
   "metadata": {},
   "source": [
    "### Task 3: UMAP Dimensionality Reduction\n",
    "\n",
    "🎯 Use UMAP (Uniform Manifold Approximation and Projection) to project high-dimensional features into 2D for visualization.\n",
    "\n",
    "UMAP is a non-linear dimensionality reduction technique that preserves both local and global structure, making it ideal for visualizing complex high-dimensional data in 2D or 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bb69aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a function for umap reduction\n",
    "def umap_reducer(X, n_components=2, n_neighbors=15):\n",
    "    \"\"\"\n",
    "    Reduce feature matrix to lower dimensions using UMAP.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray or pd.DataFrame): Feature matrix\n",
    "        n_components (int): Number of dimensions to reduce to (default=2)\n",
    "        n_neighbors (int): Number of neighbors for UMAP (default=15)\n",
    "\n",
    "    Returns:\n",
    "        X_umap (np.ndarray): UMAP-transformed features\n",
    "    \"\"\"\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90252b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><span style=\"font-size:20px; color:darkgoldenrod; font-weight:bold;\">Click to see the solution</span></summary>\n",
    "\n",
    "```python\n",
    "def umap_reducer(X, n_components=2, n_neighbors=15):\n",
    "    \"\"\"\n",
    "    Reduce feature matrix to lower dimensions using UMAP.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray or pd.DataFrame): Feature matrix\n",
    "        n_components (int): Number of dimensions to reduce to (default=2)\n",
    "        n_neighbors (int): Number of neighbors for UMAP (default=15)\n",
    "\n",
    "    Returns:\n",
    "        X_umap (np.ndarray): UMAP-transformed features\n",
    "    \"\"\"\n",
    "    # Fit UMAP and transform features\n",
    "    reducer = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, random_state=42)\n",
    "    X_umap = reducer.fit_transform(X)\n",
    "    \n",
    "    return X_umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b476ed5",
   "metadata": {},
   "source": [
    "### Task 4: Visualization \n",
    "\n",
    "Plot the UMAP projections colored by labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad26d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap(X_umap, y, target_label, dataset_name):\n",
    "    \"\"\"\n",
    "    Plot 2D UMAP projection colored by target labels.\n",
    "\n",
    "    Args:\n",
    "        X_umap (np.ndarray): UMAP-transformed features\n",
    "        y (array-like): target labels for coloring\n",
    "        target_label (str): name of the target \n",
    "        dataset_name (str): dataset name for the title (e.g., 'Video', 'Audio')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        # Scatter plot with labels as colors\n",
    "        sns.scatterplot(\n",
    "            x=X_umap[:, 0], \n",
    "            y=X_umap[:, 1], \n",
    "            hue=y, \n",
    "            palette=\"tab10\", \n",
    "            s=50\n",
    "        )\n",
    "        plt.title(f\"{dataset_name} Features UMAP - Colored by {target_label.capitalize()}\")\n",
    "        plt.xlabel(\"UMAP 1\")\n",
    "        plt.ylabel(\"UMAP 2\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Could not plot\")\n",
    "        print(\"Most likely the code is not yet complete (complete the cells with TODO).\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb94fd9",
   "metadata": {},
   "source": [
    "### Task 5: Apply UMAP to Combined Features\n",
    "\n",
    "Cluster by condition (emotion) and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "323f5ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Could not plot\n",
      "Most likely the code is not yet complete (complete the cells with TODO).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X_emo_combined, y_emo_combined = preprocess_data(combined_df, \"condition\")\n",
    "X_umap_combined = umap_reducer(X_emo_combined)\n",
    "plot_umap(X_umap_combined, y_emo_combined, \"condition\", \"Combined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7a608a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Could not plot\n",
      "Most likely the code is not yet complete (complete the cells with TODO).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_sex_combined, y_sex_combined = preprocess_data(combined_df, \"sex\")\n",
    "X_umap_combined = umap_reducer(X_sex_combined)\n",
    "plot_umap(X_umap_combined, y_sex_combined, \"sex\", \"Combined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8f738",
   "metadata": {},
   "source": [
    "### Insight?\n",
    "\n",
    "❓ Can we visually separate emotions like happy, sad, or angry?\n",
    "\n",
    "❓ How well can we distinguish between genders?\n",
    "\n",
    "➡️ Next, evaluate if seperating video and audio features can result in better clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8493ee",
   "metadata": {},
   "source": [
    "### Task 6: Apply UMAP to Video Features\n",
    "\n",
    "Cluster by condition (emotion) and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "569ad914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Could not plot\n",
      "Most likely the code is not yet complete (complete the cells with TODO).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_emo_video, y_emo_video = preprocess_data(video_df, \"condition\")\n",
    "X_umap_video = umap_reducer(X_emo_video)\n",
    "plot_umap(X_umap_video, y_emo_video, \"condition\", \"Video\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "390579e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Could not plot\n",
      "Most likely the code is not yet complete (complete the cells with TODO).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_sex_video, y_sex_video = preprocess_data(video_df, \"sex\")\n",
    "X_umap_video = umap_reducer(X_sex_video)\n",
    "plot_umap(X_umap_video, y_sex_video, \"sex\", \"Video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db094c64",
   "metadata": {},
   "source": [
    "### Insight?\n",
    "\n",
    "🤔 Do video features alone make it easier or harder to separate emotions or genders?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2082320f",
   "metadata": {},
   "source": [
    "### Task 7: Apply UMAP to Audio Features\n",
    "\n",
    "Cluster by condition (emotion) and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d7e8468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Could not plot\n",
      "Most likely the code is not yet complete (complete the cells with TODO).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X_emo_audio, y_emo_audio = preprocess_data(audio_df, \"condition\")\n",
    "X_umap_audio = umap_reducer(X_emo_audio)\n",
    "plot_umap(X_umap_audio, y_emo_audio, \"condition\", \"Audio\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36ff97e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Could not plot\n",
      "Most likely the code is not yet complete (complete the cells with TODO).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "X_sex_audio, y_sex_audio = preprocess_data(audio_df, \"sex\")\n",
    "X_umap_audio = umap_reducer(X_sex_audio)\n",
    "plot_umap(X_umap_audio, y_sex_audio, \"sex\", \"Audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e87d3c",
   "metadata": {},
   "source": [
    "### Insight?\n",
    "\n",
    "🤔 Do audio features alone make it easier or harder to separate emotions or genders?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
